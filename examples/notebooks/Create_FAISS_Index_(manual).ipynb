{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb8caf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from os import listdir, path, chdir\n",
    "\n",
    "chdir(\"..\")\n",
    "chdir(\"..\")\n",
    "\n",
    "from utils import embeddings\n",
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a0aad6",
   "metadata": {},
   "source": [
    "#### Identify dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b99cef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"LLM Powered Autonomous Agents _ Lil'Log.pdf\", \"Diffusion Models for Video Generation _ Lil'Log.pdf\", \"Adversarial Attacks on LLMs _ Lil'Log.pdf\", \"Thinking about High-Quality Human Data _ Lil'Log.pdf\", \"Extrinsic Hallucinations in LLMs _ Lil'Log.pdf\"]\n"
     ]
    }
   ],
   "source": [
    "PDF_DIR = \"./assets/pdf\"\n",
    "\n",
    "all_files = listdir(PDF_DIR)\n",
    "pdf_files = [file for file in all_files if file.endswith(\".pdf\")]\n",
    "print(pdf_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bd1e54",
   "metadata": {},
   "source": [
    "#### Read pdf content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86053947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_files(file_path: str):\n",
    "    for file_name in pdf_files:\n",
    "        file_path = path.join(PDF_DIR, file_name)\n",
    "        loader = PyPDFLoader(file_path=file_path)\n",
    "        docs = loader.load()\n",
    "        yield file_name, docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cabfc9c",
   "metadata": {},
   "source": [
    "#### Define index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aec6ec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(len(embeddings.embed_query(\"Hi\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678e2d37",
   "metadata": {},
   "source": [
    "#### Ensure `index_to_docstore` is preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f9c4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "class JsonDictManager:\n",
    "    data = {}\n",
    "\n",
    "    def __init__(self, file_path):\n",
    "        \"\"\"\n",
    "        Initialize by loading JSON from a file into the class variable.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to the JSON file\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                with open(file_path, 'r') as file:\n",
    "                    JsonDictManager.data = json.load(file)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "                JsonDictManager.data = {}\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file: {e}\")\n",
    "                JsonDictManager.data = {}\n",
    "        else:\n",
    "            print(f\"File {file_path} not found. Initializing empty dictionary.\")\n",
    "            JsonDictManager.data = {}\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"\n",
    "        Serialize the class dictionary to the JSON file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(self.file_path, 'w') as file:\n",
    "                json.dump(JsonDictManager.data, file, indent=4)\n",
    "            print(f\"Dictionary saved to {self.file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to file: {e}\")\n",
    "\n",
    "    @classmethod\n",
    "    def update_dict(cls, key, value):\n",
    "        \"\"\"\n",
    "        Update the class dictionary with a new key-value pair.\n",
    "        \n",
    "        Args:\n",
    "            key: Key to add or update\n",
    "            value: Value to associate with the key\n",
    "        \"\"\"\n",
    "        cls.data[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8680dec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ./assets/faiss_index/base_docstore.json not found. Initializing empty dictionary.\n"
     ]
    }
   ],
   "source": [
    "index_to_docstore = JsonDictManager(file_path=\"./assets/faiss_index/base_docstore.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4877e10",
   "metadata": {},
   "source": [
    "#### Define Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef565c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id=index_to_docstore.data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5c227e",
   "metadata": {},
   "source": [
    "#### Add documents to Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc180c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faiss Index not found\n",
      "Indexing documents for LLM Powered Autonomous Agents _ Lil'Log.pdf\n",
      "Indexing documents for Diffusion Models for Video Generation _ Lil'Log.pdf\n",
      "Indexing documents for Adversarial Attacks on LLMs _ Lil'Log.pdf\n",
      "Indexing documents for Thinking about High-Quality Human Data _ Lil'Log.pdf\n",
      "Indexing documents for Extrinsic Hallucinations in LLMs _ Lil'Log.pdf\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    vector_store.load_local(\"./assets/faiss_index/base\" , embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "except Exception as e:\n",
    "    print(f\"Faiss Index not found\")\n",
    "    for file_name, docs in load_pdf_files(file_path=PDF_DIR):\n",
    "        print(f\"Indexing documents for {file_name}\")\n",
    "        uuids = [uuid4().hex for _ in range(len(docs))]\n",
    "        vector_store.add_documents(documents=docs, ids=uuids)\n",
    "\n",
    "    vector_store.save_local(\"./assets/faiss_index/base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d75a66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index_to_docstore.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e9b4c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='af59975e7b7c46eca715ea2461814949', metadata={'producer': 'Skia/PDF m137', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36', 'creationdate': '2025-06-18T11:50:38+00:00', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'moddate': '2025-06-18T11:50:38+00:00', 'source': \"./assets/pdf/LLM Powered Autonomous Agents _ Lil'Log.pdf\", 'total_pages': 23, 'page': 1, 'page_label': '2'}, page_content='Figure 1: Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and\\nplan ahead.\\nTask Decomposition\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for\\nenhancing model performance on complex tasks. The model is instructed to “think step by\\nstep” to utilize more test-time computation to decompose hard tasks into smaller and simpler\\nsteps. CoT transforms big tasks into multiple manageable tasks and shed lights into an\\ninterpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at\\neach step. It first decomposes the problem into multiple thought steps and generates multiple\\nthoughts per step, creating a tree structure. The search process can be BFS (breadth-first\\nsearch) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or\\nmajority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\" ,\\n\"What are the subgoals for achieving XYZ?\" , (2) by using task-specific instructions; e.g. \"Write\\na story outline.\"  for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external\\nclassical planner to do long-horizon planning. This approach utilizes the Planning Domain\\nDefinition Language (PDDL) as an intermediate interface to describe the planning problem. In\\nthis process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical\\nplanner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates\\n6/18/25, 5:20 PM LLM Powered Autonomous Agents | Lil\\'Log\\nhttps://lilianweng.github.io/posts/2023-06-23-agent/ 2/23'),\n",
       " Document(id='d357e9e264b0465ead2957b15948871a', metadata={'producer': 'Skia/PDF m137', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36', 'creationdate': '2025-06-18T11:50:38+00:00', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'moddate': '2025-06-18T11:50:38+00:00', 'source': \"./assets/pdf/LLM Powered Autonomous Agents _ Lil'Log.pdf\", 'total_pages': 23, 'page': 10, 'page_label': '11'}, page_content='HuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select\\nmodels available in HuggingFace platform according to the model descriptions and summarize\\nthe response based on the execution results.\\nFigure 11: Illustration of how HuggingGPT works. (Image source: Shen et al.\\n2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks.\\nThere are four attributes associated with each task: task type, ID, dependencies, and\\narguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\",\\ntask_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL,\\n\"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous\\ntask which generates a new resource that the current task relies on. A\\nspecial tag \"-task_id\" refers to the generated text image, audio and video in\\nthe dependency task with id as task_id. The task MUST be selected from the\\nfollowing options: {{ Available Task List }}. There is a logical relationship\\nbetween tasks, please note their order. If the user input can\\'t be parsed,\\nyou need to reply empty JSON. Here are several cases for your reference: {{\\nDemonstrations }}. The chat history is recorded as {{ Chat History }}. From\\nthis chat history, you can find the path of the user-mentioned resources for\\nyour task planning.\\n(2) Model selection: LLM distributes the tasks to expert models, where the request is framed\\nas a multiple-choice question. LLM is presented with a list of models to choose from. Due to\\n6/18/25, 5:20 PM LLM Powered Autonomous Agents | Lil\\'Log\\nhttps://lilianweng.github.io/posts/2023-06-23-agent/ 11/23'),\n",
       " Document(id='5bba1036850343058d7fed8cc478f299', metadata={'producer': 'Skia/PDF m137', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36', 'creationdate': '2025-06-18T11:50:38+00:00', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'moddate': '2025-06-18T11:50:38+00:00', 'source': \"./assets/pdf/LLM Powered Autonomous Agents _ Lil'Log.pdf\", 'total_pages': 23, 'page': 2, 'page_label': '3'}, page_content=\"the PDDL plan back into natural language. Essentially, the planning step is outsourced to an\\nexternal tool, assuming the availability of domain-specific PDDL and a suitable planner which is\\ncommon in certain robotic setups but not in many other domains.\\nSelf-Reflection\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining\\npast action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks\\nwhere trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action\\nspace to be a combination of task-specific discrete actions and the language space. The former\\nenables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter\\nprompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nFigure 2: Examples of reasoning trajectories for knowledge-intensive tasks\\n(e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env,\\nWebShop). (Image source: Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct  works\\nbetter than the Act -only baseline where Thought: …  step is removed.\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n6/18/25, 5:20 PM LLM Powered Autonomous Agents | Lil'Log\\nhttps://lilianweng.github.io/posts/2023-06-23-agent/ 3/23\"),\n",
       " Document(id='86360b28dd1b442fbd7bba8ae07b6fce', metadata={'producer': 'Skia/PDF m137', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36', 'creationdate': '2025-06-18T11:50:38+00:00', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'moddate': '2025-06-18T11:50:38+00:00', 'source': \"./assets/pdf/LLM Powered Autonomous Agents _ Lil'Log.pdf\", 'total_pages': 23, 'page': 11, 'page_label': '12'}, page_content='the limited context length, task type based filtration is needed.\\nInstruction:\\nGiven the user request and the call command, the AI assistant helps the user\\nto select a suitable model from a list of models to process the user request.\\nThe AI assistant merely outputs the model id of the most appropriate model.\\nThe output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your\\ndetail reason for the choice\". We have a list of models for you to choose\\nfrom {{ Candidate Models }}. Please select one model from the list.\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\nWith the input and the inference results, the AI assistant needs to describe\\nthe process and results. The previous stages can be formed as - User Input:\\n{{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model\\nAssignment }}, Task Execution: {{ Predictions }}. You must first answer the\\nuser\\'s request in a straightforward manner. Then describe the task process\\nand show your analysis and model inference results to the user in the first\\nperson. If inference results contain a file path, must tell the user the\\ncomplete file path.\\n(4) Response generation: LLM receives the execution results and provides summarized results\\nto users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency\\nimprovement is needed as both LLM inference rounds and interactions with other models slow\\ndown the process; (2) It relies on a long context window to communicate over complicated\\ntask content; (3) Stability improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented\\nLLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and\\n264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse,\\nincluding search engines, calculator, calendar queries, smart home control, schedule\\nmanagement, health data management, account authentication workflow and more. Because\\nthere are a large number of APIs, LLM first has access to API search engine to find the right API\\nto call and then uses the corresponding documentation to make a call.\\n6/18/25, 5:20 PM LLM Powered Autonomous Agents | Lil\\'Log\\nhttps://lilianweng.github.io/posts/2023-06-23-agent/ 12/23')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.similarity_search(\"task planning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faf1f908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary saved to ./assets/faiss_index/base_docstore.json\n"
     ]
    }
   ],
   "source": [
    "index_to_docstore.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
